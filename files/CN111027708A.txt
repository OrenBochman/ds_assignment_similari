Parameter communication optimization method for distributed machine learning
The invention discloses a parameter communication optimization method for distributed machine learning. According to the method, the fault-tolerant characteristic of the machine learning iteration-convergence algorithm is expanded; a dynamic finite fault tolerance characteristic is provided, a distributed machine learning parameter communication optimization strategy is realized based on the dynamic finite fault tolerance, the performance of each computing node is fully utilized by dynamically adjusting the synchronization strategy of each computing node and a parameter server in combination with a performance detection model, and the accuracy of the machine learning model is ensured; sufficient computing resources are guaranteed, and the training process of the model is not affected by dynamic changes of the distributed computing resources; a training algorithm and system hardware resources are decoupled, the process that developers manually allocate computing resources and adjust andoptimize data communication according to experience is liberated, and the expansibility and high execution efficiency of a program in various cluster environments are effectively improved. The methodcan be applied to the fields of optimization of distributed machine learning parameter communication, optimization of cluster computing performance and the like.