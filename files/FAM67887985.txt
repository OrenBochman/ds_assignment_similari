Parallel analog circuit optimization method based on genetic algorithm and machine learning
The invention relates to a parallel analog circuit optimization method based on genetic algorithm and machine learning. The invention includes global optimization based on genetic algorithm and local optimization based on machine learning. The global optimization and the local optimization are performed alternately. In the global optimization part based on the genetic algorithm, SPICE simulation and parallel computation are combined, and parallel SPICE simulation is adopted, so that the optimization efficiency is greatly improved while the precision is ensured. In a local optimization stage based on machine learning, a machine learning model is established near a global optimal point obtained by global optimization, and the machine learning model is used for replacing an SPICE simulator, so that the time cost brought by a large number of SPICE simulations is reduced. The training data for training the machine learning model is also generated through parallel SPICE simulation, and compared with serial simulation, the efficiency is obviously improved. The optimization of the analog circuit by the method achieves SPICE-level optimization precision and greatly improves optimization efficiency.