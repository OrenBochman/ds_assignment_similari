{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb49d9c6-4244-44e4-bb94-ddacd57d4450",
   "metadata": {},
   "source": [
    "scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6269cdfa-2366-4e32-b34f-38789652ff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = './data/'\n",
    "def docs():\n",
    "    for file_name in os.listdir(folder):\n",
    "        with open(folder + file_name, \"r\", encoding='utf-8') as text_file:\n",
    "            #read file to a string & split\n",
    "            yield text_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2c2edf80-7400-4726-acac-ebad291f3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install -U spacy\n",
    "\n",
    "#!python3 -m spacy download en_core_web_lg\n",
    "#!python -m spacy download en_core_web_md\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "#!pip install spacy-lookups-data\n",
    "#!python3 -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "76890892-ddeb-4e08-a894-c001cf770d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "# for i, doc in enumerate(docs()):\n",
    "#     tokens = nlp(doc)\n",
    "#     for j,token in enumerate(tokens):\n",
    "#         if i <10 and j<10:\n",
    "#             print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_, token.shape_, token.is_alpha, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a4750713-f6b5-4b1d-9288-fcde8eb04cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Method for optimizing word classification in machine learning text \n",
    "\n",
    "The invention relates to the field of data processing and machine learning classification, in particular to a method for optimizing word classification in machine learning text. The method comprises the steps that on the basis of text classification, self-defined and semantically related features are filtered out through a feature selection regulator based on a regular expression, a user customizes classification types in training data after feature selection, and classification based training is conducted by means of the features and the types according to a naive Bayesian model; after training is completed, in the application stage, if statements conforming to the feature selection regulator exist in text needing word classification, classification is completed by combining a trained model. According to the method, the capacity of the model for processing work classification is not limited in word data in a training sample; the method can be applied to optimization and application of machine learning text work classification and derivation functions thereof.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1df458c6-29a7-40eb-b007-b57bd24ba1e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{',',\n",
       " '-',\n",
       " '.',\n",
       " ';',\n",
       " 'Method',\n",
       " 'accord',\n",
       " 'application',\n",
       " 'apply',\n",
       " 'base',\n",
       " 'basis',\n",
       " 'bayesian',\n",
       " 'capacity',\n",
       " 'classification',\n",
       " 'combine',\n",
       " 'complete',\n",
       " 'comprise',\n",
       " 'conduct',\n",
       " 'conform',\n",
       " 'customize',\n",
       " 'datum',\n",
       " 'define',\n",
       " 'derivation',\n",
       " 'exist',\n",
       " 'expression',\n",
       " 'feature',\n",
       " 'field',\n",
       " 'filter',\n",
       " 'function',\n",
       " 'invention',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'limit',\n",
       " 'machine',\n",
       " 'mean',\n",
       " 'method',\n",
       " 'model',\n",
       " 'naive',\n",
       " 'need',\n",
       " 'optimization',\n",
       " 'optimize',\n",
       " 'particular',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'regular',\n",
       " 'regulator',\n",
       " 'relate',\n",
       " 'related',\n",
       " 'sample',\n",
       " 'selection',\n",
       " 'self',\n",
       " 'semantically',\n",
       " 'stage',\n",
       " 'statement',\n",
       " 'step',\n",
       " 'text',\n",
       " 'thereof',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'user',\n",
       " 'word',\n",
       " 'work'}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def spacy_tokenizer_lemmatizer(text,debug = False):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    #tokens = tokenizer(text)\n",
    "    \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        if re.match('\\s+',token.text):\n",
    "            if debug: \n",
    "                print('skipping ws')\n",
    "            continue    \n",
    "        elif token.is_stop is False:\n",
    "            lemma_list.append(token.lemma_)             \n",
    "            if token.text != token.lemma_:\n",
    "                if debug: print(f'+ {token.lemma_} <- {token.text}')\n",
    "                     \n",
    "            else:\n",
    "                if debug: print(f'+ {token.lemma_}')\n",
    "            continue\n",
    "        else:\n",
    "             if debug: print(f'- {token.lemma_}')\n",
    "    return set(lemma_list)\n",
    "\n",
    "spacy_tokenizer_lemmatizer(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e7f9a7cc-7f2d-49f2-81ba-f961bafdc62e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-a4c58d59ed17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mxform_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxforms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxforms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mxforms\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'english'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mstop_word_filter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "tokenize = lambda data: word_tokenize(str(data))\n",
    "to_lower = lambda texts: [x.lower() for x in texts]\n",
    "dehyphenate = lambda tokens: [part for token in tokens for part in token.split('-')]\n",
    "\n",
    "xforms = {\"ca\": 'can', \"n't\": 'not', \"'s\": ''}\n",
    "xform_map = lambda tokens, xforms: (xforms[token] if token in xforms else token for token in tokens)\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_word_filter = lambda tokens, stop: (token for token in tokens if token not in stop)\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stem = lambda tokens,stemmer : (stemmer.stem(token) for token in tokens)\n",
    "\n",
    "print(string.punctuation)\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "punctuation_filter = lambda data, table: (w.translate(table) for w in data)\n",
    "\n",
    "len_filter = lambda tokens, n : (token for token in tokens if len(token) > n )\n",
    "\n",
    "def preprocess(doc:str) -> set:\n",
    "    \"\"\"\n",
    "    preprocessing pipeline\n",
    "    takes for a document\n",
    "    might be better converting puntuation to tokens like , to COMMA '''\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = tokenize(doc)\n",
    "    tokens = dehyphenate(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = xform_map(tokens,xforms)\n",
    "    tokens = stop_word_filter(tokens,stop_words)\n",
    "    tokens = stem(tokens,stemmer)\n",
    "    tokens = punctuation_filter(tokens,table)\n",
    "    tokens = len_filter(tokens,1)\n",
    "    return tokens\n",
    "\n",
    "def preprocess_spacy(doc:str)-> set:\n",
    "    \"\"\"\n",
    "    preprocessing pipeline with better handling of stop words\n",
    "    \"\"\"\n",
    "    tokens = spacy_tokenizer_lemmatizer(doc)\n",
    "    tokens = dehyphenate(tokens)\n",
    "    tokens = to_lower(tokens)\n",
    "    tokens = xform_map(tokens,xforms)\n",
    "    tokens = stem(tokens,stemmer)\n",
    "    tokens = punctuation_filter(tokens,table)\n",
    "    tokens = len_filter(tokens,1)\n",
    "    return tokens\n",
    "\n",
    "\n",
    "print(preprocess(sample_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bdaa25cc-f265-4ecc-938f-7fa5055c52ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP compound X.X. False False\n",
      "startup startup NOUN NN dobj xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ace737-2341-458a-a419-185788724c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def buildVocabulary(pipline,vocabulary):\n",
    "    \"\"\"\n",
    "    build vocabulary\n",
    "    \"\"\" \n",
    "    #open text file in read mode \n",
    "    doc_bar=tqdm(docs())\n",
    "    for i,doc in enumerate(doc_bar):\n",
    "        #read whole file to a string\n",
    "        words = pipline(doc)\n",
    "        vocabulary.update(words)\n",
    "        doc_bar.set_description(f\"vocab size = {len(vocabulary)} docs= {i+1}\")\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d32d9b-8897-494c-b847-ced9ba48e4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a63931c-28d7-45f6-b904-28d332b47021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
