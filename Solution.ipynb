{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc23e638-6885-4786-ba28-73a6e68ebd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simlari Assignment - TFIDF keyword extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65dec5a1-1796-4fae-b2ee-b552832587be",
   "metadata": {},
   "source": [
    "Task:\n",
    "\n",
    "Implement keyword extraction to extract other interesting keywords / phrases / concepts, which are as common as possible in all the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61fb6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "from tqdm.notebook import tqdm\n",
    "from pprint import pprint  # pretty-printer\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "#print(stopwords.words('english'))\n",
    "import spacy\n",
    "import re\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "#nlp = spacy.load(\"en_core_web_lg\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from math import log\n",
    "\n",
    "\n",
    "#nltk.download('stopwords',quiet=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028d1274-4980-42fa-a621-86086cf34d6a",
   "metadata": {},
   "source": [
    "# File generators\n",
    "let us make a file generator `docs` to allow us to operate on one file at a time in memory\n",
    "since patents documents can be large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34d870b3-97d7-4fc5-8ca7-f0da923972ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder = './files/'\n",
    "def named_docs():\n",
    "    \"\"\" a generator for files in a folder. \"\"\" \n",
    "    for file_name in os.listdir(folder):\n",
    "        with open(folder + file_name, \"r\", encoding='utf-8') as text_file:\n",
    "            #read file to a string & split\n",
    "            yield (file_name,text_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bfa3e8-801c-4dc4-84a8-c823ff2f8aa1",
   "metadata": {},
   "source": [
    "lets also make a test data set with the following ranks:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03e7836c-bf16-4b96-922b-bc647ddbaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_docs_static():\n",
    "    \"\"\" a generator for some static files - for tests \"\"\" \n",
    "    fakes = [\n",
    "        ('d01.txt','Alice likes Mr Red who sat on the river bank'),\n",
    "        ('d02.txt','Bob hates Jack Black'),\n",
    "        ('d03.txt','Alice misses Charlie Brown but she hits Bob'),\n",
    "        ('d04.txt','Bob misses Charlie Brown'),\n",
    "        ('d05.txt','Alice punched Bob and Charlie Brown black and blue'),\n",
    "        ('d06.txt','Alice Bob and Charlie robbed the bank'),\n",
    "    ]\n",
    "    \n",
    "    for fake in fakes:\n",
    "        yield fake        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e072796-2571-4190-a809-413685035c93",
   "metadata": {},
   "source": [
    "let's put this generator to use by counting the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5c836203-a016-48bc-a264-d9a25331e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "static file count=6\n",
      "real file count=100\n"
     ]
    }
   ],
   "source": [
    "def file_counter(generator):\n",
    "    file_count=0\n",
    "    for (file_name,doc) in generator():\n",
    "        file_count += 1\n",
    "    return file_count\n",
    "\n",
    "file_count=file_counter(named_docs_static)\n",
    "print(f'static file count={file_count}')\n",
    "file_count=file_counter(named_docs)\n",
    "print(f'real file count={file_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33def9f8-1c16-42e5-98d9-9eb3c940a3ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ [Alice]/PROPN\n",
      "+ [like <- likes]/VERB\n",
      "+ [Mr]/PROPN\n",
      "+ [Red]/PROPN\n",
      "- [who]/PRON  stop word\n",
      "+ [sit <- sat]/VERB\n",
      "- [on]/ADP  stop word\n",
      "- [the]/DET  stop word\n",
      "+ [river]/NOUN\n",
      "+ [bank]/NOUN\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer_lemmatizer(text,debug = False):\n",
    "    \"\"\"\n",
    "    tokenize the text, \n",
    "    drop white space token\n",
    "    drop stop words contextually\n",
    "    normalize words to lemma form    \n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    #tokens = tokenizer(text)\n",
    "    \n",
    "    lemma_list = []\n",
    "    for token in doc:\n",
    "        if re.match('\\s+',token.text):\n",
    "            if debug: \n",
    "                print(f'- [\\t\\t] ws')\n",
    "            continue \n",
    "        if re.match(f'^[{string.punctuation}]+$',token.text):\n",
    "            if debug: \n",
    "                print(f'- [{token.lemma_}] punctuation')\n",
    "            continue\n",
    "        if re.match(f'^[0-9]+^',token.text):\n",
    "            if debug: \n",
    "                print('+ {token.lemma_} <- {token.text}')\n",
    "            lemma_list.append(\"__number__\")             \n",
    "            continue         \n",
    "\n",
    "        elif token.is_stop is False:\n",
    "            lemma_list.append(token.lemma_.lower())\n",
    "#             if token.text =='learn' :\n",
    "#                 print(\"***\")\n",
    "            if token.text != token.lemma_:\n",
    "                if debug: print(f'+ [{token.lemma_} <- {token.text}]/{token.pos_}')                     \n",
    "            else:\n",
    "                if debug: print(f'+ [{token.lemma_}]/{token.pos_}')\n",
    "            continue\n",
    "        else:\n",
    "             if debug: print(f'- [{token.lemma_}]/{token.pos_}  stop word')\n",
    "             \n",
    "    return lemma_list\n",
    "\n",
    "def inspect_spacy(doc_filter=[0]):\n",
    "    for num, (_,doc) in enumerate(named_docs_static()):\n",
    "        if num in doc_filter:\n",
    "            spacy_tokenizer_lemmatizer(doc,True)\n",
    "            \n",
    "inspect_spacy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a07a68-73d5-4caa-9955-2d3524885cf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TF-IDF\n",
    "\n",
    "It possible to do TDIDF in steps or in one function. \n",
    "I like short function but I ended one long function, but keeping the data from each step.\n",
    "\n",
    "I added tokenizeation using spacy with the above method.\n",
    "I also preferred lemmatization to stemming. These are less important with embedding and larger corpus\n",
    "but we only have 100 docs.\n",
    "And I added n-gram support to process phrases using NLTK.\n",
    "\n",
    "now let's build a vocabulary, count all the term frequencies per document AKA TF, the the n and document frequencies a.k.a DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d17a9e59-4b8c-40dd-a5ea-82b1ceba402f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id={}   # numeric repersentation for each file\n",
    "def scanner(generator,max_ngrams=3):\n",
    "    \"\"\"\n",
    "    scans files and returns:\n",
    "    doc_ids   - doc to number lookup\n",
    "    term_ids  - term to number lookup\n",
    "    term_freq - count of (doc,term)\n",
    "    doc_freq  - count of docs per term\n",
    "    \"\"\"\n",
    "    doc_ids=defaultdict(int)     # map: file        =>  index (1...N)\n",
    "    term_ids=defaultdict(int)    # map: token       =>  index (1...M)\n",
    "    term_freq=defaultdict(int)   # map: (doc,token) => count\n",
    "    doc_freq=defaultdict(int)    # map: token       => doc_count\n",
    "    idf = defaultdict(int)       # map: doc_count   => log( N+1 ) - log(doc_count+1) \n",
    "    tfidf = defaultdict(int)     # map: term_freq   => term_freq * idf\n",
    "    # tdidf can inflate for long docs \n",
    "    doc_lengh = defaultdict(int) # map: file        =>  len(doc)\n",
    "    tfidf_norm = defaultdict(int)# map: tdidf       =>  tdidf/len(doc) \n",
    "    \n",
    "    #flags\n",
    "    use_ngrams=True\n",
    "    use_spacy=True\n",
    "    \n",
    "    term_counter=0\n",
    "    # once per document\n",
    "    for doc_counter, (f_name,doc) in enumerate(tqdm(generator())):\n",
    "        # let's enumerate the docs\n",
    "        doc_ids[f_name] = doc_counter\n",
    "        \n",
    "        # lets tokenize\n",
    "        doc_tokens = [] # words and phrases \n",
    "        #tokenize using spacy\n",
    "        if use_spacy:\n",
    "            doc_tokens = spacy_tokenizer_lemmatizer(doc)\n",
    "        else:\n",
    "            doc_tokens = doc.lower().split()\n",
    "          \n",
    "        doc_ngrams = [] #n-grams\n",
    "        for ngram_size in range(1,max_ngrams+1):\n",
    "            for ngram in ngrams(doc_tokens,ngram_size):\n",
    "                doc_ngrams.append(\" \".join(ngram))\n",
    "        if use_ngrams:\n",
    "            doc_tokens=doc_ngrams  \n",
    "        \n",
    "        # lets keep track of document length so we can normalize\n",
    "        doc_lengh[doc_counter] = len(doc_tokens)+1\n",
    "        \n",
    "        # for each token in the doc\n",
    "        for token in doc_tokens:\n",
    "            # let's enumerate new tokens\n",
    "            if token not in term_ids:\n",
    "                term_ids[token] = term_counter\n",
    "                term_counter += 1\n",
    "                \n",
    "            # let's build term_frequencies\n",
    "            ###############################            \n",
    "            term_freq[doc_counter,term_ids[token]] += 1\n",
    "            \n",
    "        # just once per token in the document  \n",
    "        for token in set(doc_tokens):\n",
    "            # build document frequencies\n",
    "            ############################\n",
    "            doc_freq[term_ids[token]]+=1\n",
    "            \n",
    "    # number of docs based on the counter which starts at 0\n",
    "    N = doc_counter + 1\n",
    "    # let us calculate the idf\n",
    "    for k,v in doc_freq.items():\n",
    "        idf[k] = 1 + log(N) - log(v + 1)\n",
    "        \n",
    "    # finaly let us calculate tf-idf and tfidf_norm\n",
    "    for k,v in term_freq.items():\n",
    "        tf_doc_id=k[0]\n",
    "        tf_term  =k[1]\n",
    "        tfidf[k]     = v * idf[tf_term]\n",
    "        tfidf_norm[k]= v * idf[tf_term] / doc_lengh[tf_doc_id]\n",
    "        \n",
    "    return ({\"doc_ids\" : doc_ids,        \n",
    "             \"term_ids\" : term_ids,\n",
    "             \"doc_freq\" : doc_freq,\n",
    "             \"idf\" : idf,\n",
    "             \"term_freq\" : term_freq,\n",
    "             \"tfidf\" : tfidf,\n",
    "             \"tfidf_norm\" : tfidf_norm,\n",
    "             \"doc_lengh\" : doc_lengh\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5da7e3b1-f5b4-4c84-b195-3ad8ac1dd4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a34d98388f2441897f1f6334037962b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "named_docs :\n",
      "doc_ids \t: [('CN110211379A.txt', 0), ('FAM63678824.txt', 1)] ... [('CN113496457A.txt', 98), ('US20200311599A1.txt', 99)]\n",
      "term_ids \t: [('public', 0), ('transport', 1)] ... [('send order base', 13574), ('order base partially', 13575)]\n",
      "doc_freq \t: [(104, 1), (128, 1)] ... [(13511, 1), (13558, 1)]\n",
      "idf \t: [(104, 4.912023005428146), (128, 4.912023005428146)] ... [(13511, 4.912023005428146), (13558, 4.912023005428146)]\n",
      "term_freq \t: [((0, 0), 4), ((0, 1), 4)] ... [((99, 13574), 1), ((99, 13575), 1)]\n",
      "tfidf \t: [((0, 0), 18.026231589279927), ((0, 1), 19.648092021712586)] ... [((99, 13574), 4.912023005428146), ((99, 13575), 4.912023005428146)]\n",
      "tfidf_norm \t: [((0, 0), 0.07769927409172382), ((0, 1), 0.08469005181772667)] ... [((99, 13574), 0.025450896401182106), ((99, 13575), 0.025450896401182106)]\n",
      "doc_lengh \t: [(0, 232), (1, 238)] ... [(98, 175), (99, 193)]\n"
     ]
    }
   ],
   "source": [
    "generators = [\n",
    "   # named_docs_static,\n",
    "    named_docs,    \n",
    "]\n",
    "\n",
    "# inspect \n",
    "def inspect(generator,max_idx=2):\n",
    "    \"\"\"\n",
    "        inspecct the first max_idx results in the ds returned by scanner on a generator\n",
    "\n",
    "        generator - a named file generator\n",
    "        max_idx - how many items to slice from the results\n",
    "    \"\"\"\n",
    "    results = scanner(generator) \n",
    "    print(generator.__name__,\":\")\n",
    "    for name, data_dict in results.items():\n",
    "        print(f'{name} \\t: {list(data_dict.items())[0:max_idx]} ... {list(data_dict.items())[-max_idx:]}')\n",
    "        \n",
    "    # since the term are set to term ids we might want to see the terms\n",
    "    # TODO print ds with terms instead of ids\n",
    "    \n",
    "    return results\n",
    "\n",
    "for generator in generators:    \n",
    "    results=inspect(generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a5716c-e9dd-4569-a489-3029e5af0a84",
   "metadata": {},
   "source": [
    "we can now:\n",
    "1. [ ] rank terms and phrases per document.\n",
    "1. [ ] rank phrases using total tf-idf scores on the corpus\n",
    "1. [ ] move on to embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "990a774b-3564-4c2d-bc2b-e17db8c593b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_ids = results[\"term_ids\"]\n",
    "id_terms = {v:k for (k,v) in term_ids.items() }\n",
    "\n",
    "#term_ids\n",
    "#id_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b36598-af98-4751-bc78-1995ec6c0963",
   "metadata": {},
   "source": [
    "before ranking term we should split them into unigrams, bigram and trigram.etc\n",
    "this will allow us to pick the top items of each speretly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09c9eb9e-c81a-41f1-8fa3-e4be2bf53460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams: 1267\n",
      "first 30 unigrams are:\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "bigrams: 5297\n",
      "first 30 bigrams are:\n",
      "[42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71]\n",
      "trigrams: 7012\n",
      "first 30 trigrams are:\n",
      "[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132]\n"
     ]
    }
   ],
   "source": [
    "unigrams=[]\n",
    "bigrams=[]\n",
    "trigrams=[]\n",
    "\n",
    "for term,id in term_ids.items():\n",
    "    tuple_size = len(term.split())\n",
    "    if tuple_size == 1:\n",
    "        unigrams.append(id)\n",
    "    if tuple_size == 2:\n",
    "        bigrams.append(id)\n",
    "    if tuple_size == 3:\n",
    "        trigrams.append(id)\n",
    "    if False:\n",
    "        print(f'id:{id:4},length: {len(term):3},tokens:{len(term.split()):3}, term: {term} ')\n",
    "\n",
    "ngrams_dict = {\"unigrams\":unigrams,\"bigrams\":bigrams,\"trigrams\":trigrams}\n",
    "\n",
    "def inspect_ngrams(slice=30):    \n",
    "    for (k,v) in ngrams_dict.items():\n",
    "        print(f\"{k}: {len(v)}\")\n",
    "        print(f'first {slice} {k} are:')\n",
    "        print(v[0:slice])\n",
    "\n",
    "inspect_ngrams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f382e9b-f5d3-4c53-825a-9f1a6e2d0ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "term_rank = defaultdict(int)\n",
    "for k,v in results[\"tfidf_norm\"].items():\n",
    "    doc=k[0]\n",
    "    term=k[1]\n",
    "    term_rank[term] += v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d4399df6-d590-4cf2-8584-21b5b99db583",
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "sorted_term_rank = sorted(term_rank.items(), key=operator.itemgetter(1),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "300cc9ad-cdf0-45a6-b854-fac07ae8f949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigrams score: 1.76 \t term: machine\n",
      "unigrams score: 1.33 \t term: model\n",
      "unigrams score: 1.21 \t term: optimization\n",
      "unigrams score: 1.08 \t term: learning\n",
      "unigrams score: 1.04 \t term: parameter\n",
      "unigrams score: 1.02 \t term: method\n",
      "unigrams score: 0.96 \t term: learn\n",
      "unigrams score: 0.89 \t term: datum\n",
      "unigrams score: 0.81 \t term: system\n",
      "unigrams score: 0.80 \t term: device\n",
      "unigrams score: 0.72 \t term: base\n",
      "unigrams score: 0.68 \t term: network\n",
      "unigrams score: 0.67 \t term: set\n",
      "unigrams score: 0.66 \t term: value\n",
      "unigrams score: 0.62 \t term: algorithm\n",
      "unigrams score: 0.57 \t term: plurality\n",
      "unigrams score: 0.57 \t term: hyper\n",
      "unigrams score: 0.56 \t term: optimize\n",
      "unigrams score: 0.54 \t term: configuration\n",
      "unigrams score: 0.53 \t term: search\n",
      "unigrams score: 0.52 \t term: training\n",
      "unigrams score: 0.48 \t term: optimal\n",
      "unigrams score: 0.48 \t term: include\n",
      "unigrams score: 0.47 \t term: obtain\n",
      "unigrams score: 0.45 \t term: process\n",
      "unigrams score: 0.44 \t term: operation\n",
      "unigrams score: 0.44 \t term: state\n",
      "unigrams score: 0.42 \t term: information\n",
      "unigrams score: 0.42 \t term: time\n",
      "unigrams score: 0.41 \t term: prediction\n",
      "unigrams score: 0.40 \t term: invention\n",
      "unigrams score: 0.40 \t term: hyperparameter\n",
      "unigrams score: 0.40 \t term: platform\n",
      "unigrams score: 0.40 \t term: quantum\n",
      "unigrams score: 0.40 \t term: provide\n",
      "unigrams score: 0.40 \t term: accord\n",
      "unigrams score: 0.39 \t term: determine\n",
      "unigrams score: 0.38 \t term: module\n",
      "unigrams score: 0.36 \t term: object\n",
      "\n",
      "bigrams score: 0.95 \t term: machine learning\n",
      "bigrams score: 0.90 \t term: machine learn\n",
      "bigrams score: 0.58 \t term: hyper parameter\n",
      "bigrams score: 0.57 \t term: learning model\n",
      "bigrams score: 0.53 \t term: learn model\n",
      "bigrams score: 0.44 \t term: optimization method\n",
      "bigrams score: 0.41 \t term: learn algorithm\n",
      "bigrams score: 0.35 \t term: learning method\n",
      "bigrams score: 0.34 \t term: base machine\n",
      "bigrams score: 0.33 \t term: parameter value\n",
      "bigrams score: 0.24 \t term: handover parameter\n",
      "bigrams score: 0.24 \t term: circuit configuration\n",
      "bigrams score: 0.24 \t term: optimization problem\n",
      "bigrams score: 0.23 \t term: parameter optimization\n",
      "bigrams score: 0.23 \t term: method base\n",
      "bigrams score: 0.23 \t term: method device\n",
      "bigrams score: 0.23 \t term: device machine\n",
      "bigrams score: 0.22 \t term: source route\n",
      "bigrams score: 0.21 \t term: learning system\n",
      "bigrams score: 0.21 \t term: terminal device\n",
      "bigrams score: 0.20 \t term: learning datum\n",
      "bigrams score: 0.20 \t term: training datum\n",
      "bigrams score: 0.20 \t term: invention disclose\n",
      "bigrams score: 0.20 \t term: optimization machine\n",
      "bigrams score: 0.20 \t term: set hyperparameter\n",
      "bigrams score: 0.19 \t term: method comprise\n",
      "bigrams score: 0.19 \t term: parameter configuration\n",
      "bigrams score: 0.19 \t term: method system\n",
      "bigrams score: 0.19 \t term: radiotherapy treatment\n",
      "bigrams score: 0.19 \t term: treatment plan\n",
      "bigrams score: 0.19 \t term: plan optimization\n",
      "bigrams score: 0.19 \t term: optimize machine\n",
      "bigrams score: 0.18 \t term: fault diagnosis\n",
      "bigrams score: 0.18 \t term: model optimization\n",
      "bigrams score: 0.18 \t term: method include\n",
      "bigrams score: 0.18 \t term: state datum\n",
      "bigrams score: 0.18 \t term: system method\n",
      "bigrams score: 0.18 \t term: storage medium\n",
      "bigrams score: 0.18 \t term: quantum pulse\n",
      "bigrams score: 0.18 \t term: datum set\n",
      "bigrams score: 0.18 \t term: fpga device\n",
      "bigrams score: 0.17 \t term: learning technique\n",
      "bigrams score: 0.17 \t term: learn device\n",
      "bigrams score: 0.17 \t term: electronic device\n",
      "bigrams score: 0.17 \t term: real time\n",
      "bigrams score: 0.17 \t term: test datum\n",
      "bigrams score: 0.16 \t term: optimization design\n",
      "bigrams score: 0.16 \t term: multivalued function\n",
      "bigrams score: 0.16 \t term: compute device\n",
      "bigrams score: 0.16 \t term: learning platform\n",
      "bigrams score: 0.16 \t term: present invention\n",
      "bigrams score: 0.15 \t term: prediction model\n",
      "bigrams score: 0.15 \t term: candidate machine\n",
      "bigrams score: 0.14 \t term: search engine\n",
      "bigrams score: 0.14 \t term: method optimize\n",
      "bigrams score: 0.14 \t term: invention relate\n",
      "bigrams score: 0.14 \t term: plurality schedule\n",
      "bigrams score: 0.13 \t term: time optimization\n",
      "bigrams score: 0.13 \t term: train machine\n",
      "bigrams score: 0.13 \t term: include machine\n",
      "bigrams score: 0.13 \t term: quantization machine\n",
      "bigrams score: 0.13 \t term: tenant machine\n",
      "bigrams score: 0.13 \t term: determination datum\n",
      "bigrams score: 0.13 \t term: comprise following\n",
      "bigrams score: 0.13 \t term: following step\n",
      "bigrams score: 0.13 \t term: search space\n",
      "bigrams score: 0.12 \t term: optimization acceleration\n",
      "bigrams score: 0.12 \t term: learning module\n",
      "bigrams score: 0.12 \t term: system computer\n",
      "bigrams score: 0.12 \t term: catheterization laboratory\n",
      "bigrams score: 0.12 \t term: measure interest\n",
      "bigrams score: 0.12 \t term: inference task\n",
      "bigrams score: 0.12 \t term: loss value\n",
      "bigrams score: 0.12 \t term: online platform\n",
      "bigrams score: 0.12 \t term: configuration optimization\n",
      "bigrams score: 0.12 \t term: optimization system\n",
      "bigrams score: 0.12 \t term: mechanical metamaterial\n",
      "bigrams score: 0.12 \t term: global optimization\n",
      "bigrams score: 0.12 \t term: computer implement\n",
      "bigrams score: 0.12 \t term: operation machine\n",
      "\n",
      "trigrams score: 0.57 \t term: machine learning model\n",
      "trigrams score: 0.53 \t term: machine learn model\n",
      "trigrams score: 0.41 \t term: machine learn algorithm\n",
      "trigrams score: 0.33 \t term: machine learning method\n",
      "trigrams score: 0.21 \t term: handover parameter value\n",
      "trigrams score: 0.21 \t term: base machine learning\n",
      "trigrams score: 0.19 \t term: radiotherapy treatment plan\n",
      "trigrams score: 0.19 \t term: treatment plan optimization\n",
      "trigrams score: 0.19 \t term: optimization machine learning\n",
      "trigrams score: 0.19 \t term: machine learning system\n",
      "trigrams score: 0.18 \t term: optimize machine learning\n",
      "trigrams score: 0.17 \t term: machine learning technique\n",
      "trigrams score: 0.17 \t term: machine learn device\n",
      "trigrams score: 0.16 \t term: plan optimization problem\n",
      "trigrams score: 0.16 \t term: device machine learning\n",
      "trigrams score: 0.16 \t term: machine learning platform\n",
      "trigrams score: 0.16 \t term: method base machine\n",
      "trigrams score: 0.16 \t term: base machine learn\n",
      "trigrams score: 0.15 \t term: candidate machine learning\n",
      "trigrams score: 0.14 \t term: optimization method device\n",
      "trigrams score: 0.13 \t term: quantization machine learn\n",
      "trigrams score: 0.13 \t term: optimization method base\n",
      "trigrams score: 0.13 \t term: comprise following step\n",
      "trigrams score: 0.12 \t term: machine learning module\n",
      "trigrams score: 0.12 \t term: method system computer\n",
      "trigrams score: 0.12 \t term: optimization method system\n",
      "trigrams score: 0.12 \t term: circuit configuration optimization\n",
      "trigrams score: 0.12 \t term: real time optimization\n",
      "trigrams score: 0.11 \t term: machine learn optimization\n",
      "trigrams score: 0.11 \t term: learn optimization method\n",
      "trigrams score: 0.11 \t term: datum machine learning\n",
      "trigrams score: 0.11 \t term: deep semantic segmentation\n",
      "trigrams score: 0.11 \t term: hyper parameter optimization\n",
      "trigrams score: 0.11 \t term: source route network\n",
      "trigrams score: 0.11 \t term: route network path\n",
      "trigrams score: 0.11 \t term: machine learn invention\n",
      "trigrams score: 0.11 \t term: multi tenant machine\n",
      "trigrams score: 0.11 \t term: time optimization computer\n",
      "trigrams score: 0.11 \t term: optimization computer implement\n",
      "trigrams score: 0.11 \t term: computer implement application\n",
      "trigrams score: 0.11 \t term: implement application operation\n",
      "trigrams score: 0.11 \t term: application operation machine\n",
      "trigrams score: 0.11 \t term: operation machine learning\n",
      "trigrams score: 0.10 \t term: set training datum\n",
      "trigrams score: 0.10 \t term: plurality terminal device\n",
      "trigrams score: 0.10 \t term: set optimal handover\n",
      "trigrams score: 0.10 \t term: optimal handover parameter\n",
      "trigrams score: 0.10 \t term: machine learn method\n",
      "trigrams score: 0.10 \t term: machine learn task\n",
      "trigrams score: 0.10 \t term: include machine learning\n",
      "trigrams score: 0.10 \t term: machine learn fund\n",
      "trigrams score: 0.10 \t term: learn fund optimization\n",
      "trigrams score: 0.10 \t term: fault diagnosis system\n",
      "trigrams score: 0.10 \t term: learning model invention\n",
      "trigrams score: 0.10 \t term: non supervised machine\n",
      "trigrams score: 0.09 \t term: computer storage medium\n",
      "trigrams score: 0.09 \t term: train route optimization\n",
      "trigrams score: 0.09 \t term: route optimization include\n",
      "trigrams score: 0.09 \t term: optimization include machine\n",
      "trigrams score: 0.09 \t term: learning module 120\n",
      "trigrams score: 0.09 \t term: optimization module 110\n",
      "trigrams score: 0.09 \t term: tenant machine learning\n",
      "trigrams score: 0.09 \t term: analysis machine learning\n",
      "trigrams score: 0.09 \t term: method comprise step\n",
      "trigrams score: 0.09 \t term: method comprise following\n",
      "trigrams score: 0.09 \t term: learn invention disclose\n",
      "trigrams score: 0.09 \t term: machine learn engine\n",
      "trigrams score: 0.09 \t term: robust optimization design\n",
      "trigrams score: 0.09 \t term: optimization design method\n",
      "trigrams score: 0.09 \t term: machine learning device\n",
      "trigrams score: 0.08 \t term: method device machine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def inspect_top_terms(slice=3):\n",
    "    #iterate over ngram size\n",
    "    for n,(ng_name,ng_dict) in enumerate(ngrams_dict.items()):\n",
    "        counter = 0\n",
    "        slicer = slice/100*len(ng_dict)/(n+1)\n",
    "        for (term_id,rank) in sorted_term_rank :\n",
    "            if counter >= slicer:\n",
    "                continue\n",
    "            elif term_id in ng_dict:\n",
    "                print(f\"{ng_name} score: {rank:0.2f} \\t term: {id_terms[term_id]}\")\n",
    "                counter +=1\n",
    "        print()\n",
    "                \n",
    "inspect_top_terms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5863f9-6f2c-46b7-ac30-cdaaa64f36d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
